### http://www.sohu.com/a/206400541_609376
传统技术已经不能适应病毒数量飞速增长的发展态势。而基于沙箱的检测方案无法满足 APT 攻击的检测需求，也受到多种反沙箱技术的干扰。
基于深度学习的二进制病毒样本检测技术，可以做到沙箱同等水平的 99% 的检测准确率，而误报率低于 1/1000。

基于内容的就是一般所谓的静态分析，病毒样本不需要实际执行起来。安全人员直接打开文件查看二进制码或者反汇编后来分析源代码都算静态分析。加壳的样本，尤其是复杂的壳，要做反汇编其实并不容易。内容结合特征码就是传统的病毒引擎的原理，我们日常在都在使用。依赖安全人员给出精准匹配的特征码，匹配迅速，但是就如我们的杀毒引擎一样，需要定期更新病毒库。

基于行为的就是一般所谓的动态分析，需要在实际或者通过虚拟化的方法把病毒样本执行起来。通过考察样本对操作系统各种资源的操作来构建特征和分析。某些大病毒家族，比如勒索软件，因为操作高调，通过动态分析非常好识别。

基于规则的方法在静态分析领域用得很少，算是补充。但是在沙箱领域非常常见，因为病毒行为很容易写出规则来。但实际的情况是病毒在沙箱内运行的时间短，最多就是 30 分钟，往往 10 分钟都不到，导致其行为暴露不够充分。

运用分类算法基于行为特征来检测看似不错，但是行为特征少是一个明显的缺陷。所以很多时候往往是混合了动态的和静态的特征来构建。

运用网络行为加算法来分析就是目前比较火也很有前途的 NTA（Network Traffic Analytics）。NTA 既融合了传统规则，也结合了机器学习，通过监测网络的流量、连接和对象来识别恶意样本产生的行为。再配合上质量好的威胁情报，能产生高信息熵的特征，特别适合 Botnet （僵尸网络）这一类的病毒。

在业界中，基于特征码（也就是内容特征匹配）的技术仍然占有压倒性优势。统计发现，只要能拿到对应的病毒样本，各大杀毒引擎厂家都能在 3 小时左右部署特征码。但这个工业流程到达极限以后，厂商在后台拿到新样本的速度很难再提高（APT 也难拿到样本），要么是拿到了也处理不过来（新病毒的产生速度一直在提高，有报告统计出每 4.2 秒就有一个新病毒产生）。一般我们普遍认为，在 APT 开始大量出现后，特征匹配的方法（流程）到了极限。
## 	微软开办了第一届恶意软件分类大赛（Microsoft Malware Classification Challenge）
微软比赛中第一名的方案是用 N-gram 产生数万特征，然后用 XGboost 来做分类。

## 为什么深度学习适用于病毒样本分类的问题:
首先是效果足够好，在图像识别、语音识别、机器翻译等领域的效果都远远超过非深度学习的算法。
第二是样本的性质合适，深度学习就是擅长处理单一类型的数据。对应我们的输入都是二进制文件。
第三要求足够多的样本，而样本越多准确的也越高。病毒的样本根据 Symantec 在今年 Q2 的统计，单日样本可以达到 300 万。
第四，可以避免人工去选择特征，只要一开始就设计好网络结构，深度学习会自动学习到重要的特征。再对比静态分析，比如用 N-gram，特征数量会轻松突破百万，再乘以上面的样本数，机器学习是必然选择。

安全系统属于机器学习的下游，一般安全领域不容易直接产生对机器学习新思路突破，经常是借鉴上游的突破（如图像识别），所以还有很多安全领域等待我们去应用新的机器学习算法。
网络结构：对于这个样本分类来说，我们前后换过几个 resnet 变种，目前部署的模型是基于 inception-v4，但目前正在实验 densenet，因为同等检测率下，参数更少计算更快。在客户环境下部署，我们希望尽量降低对硬件的需求。从我们实验的效果看，从早的 resnet 到新的 inception-v4，迁移学习后差别只在 5% 内。所以今后我们研究的重点在速度了，网络选择是第二位。

输入：第一，我们需要把二进制样本通过一种方法转换为图像。第二，我们需要借助图像分类的经验甚至是特征模型来帮助我们做病毒的分类。

迁移学习：有两个近似领域 A 和 B，通过深度学习自动提取特征以后，我们发现两个领域在浅层网络中的低级特征其实是可以共享的，而在深层网络中因为领域不同而对应了不同的高级特征。一种普遍的情况是领域 A 有大量的标记数据，有优良的特征模型已经生成。而领域 B 数据量比较小。那么通过共享特征，我们可以应用迁移学习将适用于大数据的领域模型 A 借用过来，再通过领域 B 的标签数据去训练高级特征和分类，从而在领域 B 上实现更好的分类效果。

### 流程：
将正负样本按 1：1 的比例转换为图像。将 ImageNet 中训练好的图像分类模型作为迁移学习的输入。在 GPU 集群中进行训练。我们同时训练了标准模型和压缩模型，对应不同的客户需求（有无 GPU 环境）。
训练了双模型，一个Inception-V4，一个是 Squeezenet。Inception-V4 是目前较为先进的模型，有最好的实验结果，训练和 inference 的速度也可以接受。而 Squeezenet 是压缩模型，参数数量只有 AlexNet 的 1/50，虽然准确度稍差，但检测速度快很多，专为不能提供 GPU 环境的客户设计。另外针对这两个模型做迁移学习的时候，我们都替换掉了最顶和最低几层（输入层的替换主要是对接我们转换后的样本。输出层的替换主要是分类器我们用了 lightGBM）。
（压缩模型：为了提速，我们可以通过剪枝或者选择小网络来训练，只保留几十分之一的参数，缺只损失 5% 左右的准确度。这也是最近深度学习的一个发展方向。）
测试结果，AUC 可以达到 0.985，误报率小于 1/1000，检测的速度目前可以达到 150M/ 天。

## 二进制文件转图像
常规的网络一般能输入的尺寸也就是 300 x 300 上下，也就是 9K 左右的规模。而病毒样本的大小平均接近 1M，是远远大于这个尺寸。图像领域的常规转换方法就是缩放，或者用 pyramid pooling（金字塔池）。这两者我们实验效果都非常低差，AUC 在 0.6 左右。所以后来我们又设计了一个很复杂的 pooling 算法处理大尺寸文件。
一次性转换，没有分段。（文件大小不一的处理？）

## 处理加壳：
简单的壳：A->A’，B->B’，还是可以分开A，B。复杂的壳，A,B->C’，DL学到的是C’的特征。
深度学习模型记住的是病毒二进制文件中的有效特征，而不是特征码（特征码由专家选取，对应唯一病毒样本），所以具有更好的通用性。在实际测试中，即使一个月不更新模型，对新衍生的病毒样本也有较高的识别能力。

我们的模型对恶意的 HTML 检测率也很高。但同时对正常的 HTML 样本有很高的误报率。定位发现原因是训练集的恶意样本中包含很多 HTML 内容，被深度学习抽取成了特征。优化方法很简单，只需要在正常样本中加入一定的 HTML 就可以平衡。
下一步，我们还会深入到网络中，继续探索具体的检测机制。我们还会测试其他的样本类型，比如文档类型。目前我们的输出只是一个二元判断，那安全人员可能希望可以进一步给出病毒类型，甚至是家族归属。方案层面，除了目前使用的二进制码转低维度图片 +CNN 的方法，我们也在测试另一套方案，考察二进制码在长空间跨度下的特征，并应用 LSTM。

基于真实环境中的病毒样本来做训练的。安全团队每月都会拿到千万的最新病毒样本，我们也会持续更新模型。这个最终是会应用到客户环境的，并不是一个纯研究性的课题。
